# Introduction to Dimensionality Reduction

Week 5 goals are to understand the tools and steps involved in presenting and clustering high-dimensional data.

Hopefully, you have been able to find the PREMIX metagenome file in our OneDrive directory:

- PREMIX_mg_biom.json: Biom formatted output of species level Bracken results for all metagenome files
- PREMIX_metadata.csv: Metadata file for PREMIX metagenome files

## 1. phyloseq package documentation

We didn't spend much time on the phyloseq package in Week 4 but it is a useful package to keep consistent data structures for your microbiome data and perform typical analyses. We will also use some function from the cluster package to perform our unsupervised cluster analysis.

- [phyloseq full stack](https://f1000research.com/articles/5-1492/v1)
- [cluster package documentation](https://cran.r-project.org/web/packages/cluster/cluster.pdf)

## 2. Dimensionality reduction

The goal of dimensionality reduction is a way to summarize high dimensional data (i.e. many rows, many columns) in a way that is meaningful and statistically appropriate.

## 3. Bray-Curtis PCoA Analysis in Phyloseq

We will perform these steps in phyloseq since they make it fairly simple to do so. Since phyloseq is mostly a way to consistently structure the typical components of microbiome datasets and includes many additional R packages, you could (and probably should at some point!) do this with each component step but that is outside of our scope. Some steps/code below are adapted from the [phyloseq documentation](https://joey711.github.io/phyloseq/plot_ordination-examples.html).

``` r
# load phyloseq
library(tidyverse)
library(phyloseq)
library(readr)

########### import data
# import biom file to phyloseq
# see https://www.cell.com/cms/10.1016/j.isci.2020.100905/attachment/d3d5d291-4f11-4ecf-af92-558af5d1f474/mmc7
biom_path   <-"${path_to_biom_file}"    # double check you have a "/" at the end of this value
metadata_path <- "${path_to_metadata_file}"     # double check you have a "/" at the end of this value

data        <- import_biom(paste0(biom_path,"PREMIX_mg_biom.json"), parseFunction=parse_taxonomy_default)
md          <- read_csv(paste0(metadata_path, "PREMIX_metadata.csv"))

# list of PREMIX participants who completed at least one cycle & donor
PREMIX_ID_list <- c("PM01",
                     "PM02",
                     "PM03",
                     "PM04",
                     "PM05",
                     "PM06",
                     "PM07",
                     "PM08",
                     "PM09",
                     "PM12",
                     "PM13",
                     "SD01")
```

``` r
########### prepare data
# prepare sample data (i.e. metadata) for storage in phyloseq object
sampledata  <- sample_data(md)

# pull OTU table from imported biom file
OTU         <- otu_table(data@otu_table, taxa_are_rows = TRUE)

# pull taxonomy table from imported biom file
TAX         <- tax_table(data@tax_table)

# use OTU table sample names to label sample data object
sample_names(sampledata)  <- sample_names(OTU)
```

``` r
########### make phyloseq object
physeq      <- phyloseq(TAX, sampledata, OTU)

    # label taxonomy table columns
    colnames(tax_table(physeq)) <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species")
```

``` r
########### pre-process/clean phyloseq object
# remove taxa with unassigned sequences
physeq <- subset_taxa(physeq, Genus != "-1")

# remove taxa not seen more than 2 times
physeq <- filter_taxa(physeq, function(x) sum(x > 2) > (0.2*length(x)), TRUE)
  
# standardize abundances to median sequencing depth
    total <- median(sample_sums(physeq))
    standf <- function(x, t=total) round(t * (x / sum(x)))
    
    # this step transforms counts to abundances standardized to the median
    # sequencing depth using the formulae above.
    # this should allow us to examine differences in count & compositional (proportional)
    # datasets if you substitute physeq with physeq_s in the following steps.
    physeq_s <- transform_sample_counts(physeq, standf)
```

``` r
########### calculate & plot bray-curtis distance

# calculate bray-curtis distance just for patients who completed visits/donor
visits_braycurtis <- subset_samples(physeq, ID %in% PREMIX_ID_list) %>% 
  distance(method="bray")

# export visits_braycurtis matrix (subset of just patients with visits & donor)
BCmat_visits <- as.matrix(visits_braycurtis)

# just subset with VISITS
PCoA       <- ordinate(subset_samples(physeq, ID %in% PREMIX_ID_list), method="PCoA", distance=visits_braycurtis)
```
- If you did NMDS plotting last week, do you notice huge differences with the PCoA projection?

## 4. Cluster Analysis

Partition Around Medoids (PAM) is a method to assign samples to a cluster given an optimal number of clusters.

First, we will need to determine an optimal number of clusters using silhouette anlaysis.

``` r
# calculate bray-curtis distance just for baseline samples from patients who completed visits/donor
pam_braycurtis <- subset_samples(physeq, Group %in% c("Screen","Donor") & ID %in% PREMIX_ID_list) %>% 
  distance(method="bray")
  
# silhouette analysis for clustering
library(factoextra)

# scale the braycurtis matrix we extracted from phyloseq
scaled_pam_braycurtis = scale(pam_braycurtis)

# plot the optimal number of clusters
fviz_nbclust(scaled_pam_braycurtis, cluster::pam, method = "silhouette") + theme_minimal()
```
- To how many clusters should we assign our baseline samples based on this silhouette analysis?

Next, we will need to assign our samples to the given number of clusters.
``` r
# cluster 
pamResult <- cluster::pam(pam_braycurtis, k = 2)

# add the scaled dissimilarity matrix back to the pamResult object for the fviz_cluster function
pamResult$data <- scaled_pam_braycurtis

# vizualize clusters
fviz_cluster(pamResult, 
             palette = c("#007892","#D9455F"),
             ellipse.type ="euclid",
             repel =TRUE,
             ggtheme =theme_minimal())
```
- Which participants cluster by baseline bray-curtis dissimilarity with the donor? 
- Which don't?

## t-SNE?

t-SNE is another dimensionality reduction approach that seems to have gained some popularity over the last few years. Look at Fig. 3 in [this inspiring paper from Ying Taur et al.](https://doi-org.proxy.library.emory.edu/10.1126/scitranslmed.aap9489) I have had it on my list to use for some of our functional metagenomic analysis. It should probably be beyond our scope for this course but I think is worth pointing out. It is not deterministic and will have some variability with each run but depending on your dataset, may be a way to highlight differences in your dataset. Here are a couple of sites to start to get a sense of how it works:

- [t-SNE clearly explained](https://towardsdatascience.com/t-sne-clearly-explained-d84c537f53a)
- [quick and easy t-SNE in r](https://www.r-bloggers.com/2019/05/quick-and-easy-t-sne-analysis-in-r/)

## Week 6 prep (5 minutes)

Next week, we will work on contig assembly and binning. To prepare for next week, let's:

- Read the [metaSPAdes paper](https://pubmed.ncbi.nlm.nih.gov/28298430/) 
- Read [meren's blog post about the fate of contigs](https://merenlab.org/2020/01/02/visualizing-metagenomic-bins/)
